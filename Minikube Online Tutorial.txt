---------- A. Create a Cluster ----------

1. Minikube version
$ minikube version

2. Start the cluster
$ minikube start
=> Minikube started a virtual machine for you, and a Kubernetes cluster is now running in that VM

3. See both the version of the client and as well as the server
$ kubectl version
=> The client version is the kubectl version, the server version is the Kubernetes version installed on the master.

4. View the cluster details
$ kubectl cluster-info
=> address info (Kubernetes master, KubeDNS)

5. View the nodes in the closter
$ kubectl get nodes
=> status: ready - it is ready to accept applications for deployment




---------- B. Deploy an App ----------

1. Check that kubectl is configured to talk to your cluster
$ kubectl version

2. To view the nodes in the cluster
$ kubectl get nodes
=> Kubernetes will choose where to deploy our application based on Node available resources.

3. To deploy your first app on Kubernetes with the 'kubectl create deployment' command,
you need to provide the deployment name and app image location (include the full repository url for images hosted outside Docker hub).
e.g.)
$ kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1
=> This performed a few things for you.
- searched for a suitable node where an instance of the application could be run (you can see available nodes with: kubectl get nodes)
- scheduled the application to run on that Node
- configured the cluster to reschedule the instance on a new Node when needed

4. To list your deployments
$ kubectl get deployments
=> See that there is 1 deployment running a single instance of your app. The instance is running inside a Docker container on your node.

* We will cover other options on how to expose your application outside the kubernetes cluster in Module 4.

5. Open a second terminal window to run the proxy (The kubectl command can create a proxy that will forward communications into the cluster-wide, private network. The proxy can be terminated by pressing control-C and won't show any output while its running.)
$ echo -e "\n\n\n\e[92mStarting Proxy. After starting it will not output a response. Please click the first Terminal Tab\n";
=> We now have a connection between our host (the online terminal) and the Kubernetes cluster. The proxy enables direct access to the API from these terminals.

6. You can see all those APIs hosted through the proxy endpoint. For example, we can query the version directly through the API using the curl command:
$ curl http://localhost:8001/version
(* If Port 8001 is not accessible, ensure that the kubectl proxy started above is running.)
(The proxy still runs in the second tab, and this allowed our curl command to work using localhost:8001.)

7. The API server will automatically create an endpoint for each pod, based on the pod name, that is also accessible through the proxy.
First we need to get the Pod name, and we'll store in the environment variable POD_NAME:
$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
(* A Kubernetes pod is a group of containers that are deployed together on the same host. If you frequently deploy single containers, you can generally replace the word "pod" with "container" and accurately understand the concept.)

In order for the new deployment to be accessible without using the Proxy, a Service is required which will be explained in the next modules.




---------- C. Explore Your App ----------

A Pod models an application-specific "logical host" and can contain different application containers which are relatively tightly coupled. For example, a Pod might include both the container with your Node.js app as well as a different container that feeds the data to be published by the Node.js webserver.
When we create a Deployment on Kubernetes, that Deployment creates Pods with containers inside them (as opposed to creating containers directly).
Each Pod is tied to the Node where it is scheduled, and remains there until termination (according to restart policy) or deletion. In case of a Node failure, identical Pods are scheduled on other available Nodes in the cluster.

A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. The Master's automatic scheduling takes into account the available resources on each Node.
Every Kubernetes Node runs at least:
- Kubelet, a process responsible for communication between the Kubernetes Master and the Node; it manages the Pods and the containers running on a machine.
- A container runtime (like Docker, rkt) responsible for pulling the container image from a registry, unpacking the container, and running the application.
Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.

In Module 2, you used Kubectl command-line interface. You'll continue to use it in Module 3 to get information about deployed applications and their environments. The most common operations can be done with the following kubectl commands:
$ kubectl get => list resources
$ kubectl describe => show detailed information about a resource
$ kubectl logs => print the logs from a container in a pod
$ kubectl exec => execute a command on a container in a pod



1. To look for existing pods
$ kubectl get pods

2. To view what containers are inside that Pod and what images are used to build those containers
$ kubectl describe pods

3. Recall that Pods are running in an isolated, private network - so we need to proxy access to them so we can debug and interact with them. To do this, we'll use the kubectl proxy command to run a proxy in a second terminal window. Click on the command below to automatically open a new terminal and run the proxy:
$ echo -e "\n\n\n\e[92mStarting Proxy. After starting it will not output a response. Please click the first Terminal Tab\n"; kubectl proxy

4. Now again, we'll get the Pod name and query that pod directly through the proxy. To get the Pod name and store it in the POD_NAME environment variable:
$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
$ echo Name of the Pod: $POD_NAME

5. To see the output of our application, run a curl request.
$ curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/
=> The url is the route to the API of the Pod.

6. Anything that the application would normally send to STDOUT becomes logs for the container within the Pod. We can retrieve these logs using the kubectl logs command:
$ kubectl logs $POD_NAME
(Note: We don’t need to specify the container name here, because we only have one container inside the pod.)

7. We can execute commands directly on the container once the Pod is up and running. For this, we use the exec command and use the name of the Pod as a parameter. Let’s list the environment variables:
$ kubectl exec $POD_NAME env
(Again, worth mentioning that the name of the container itself can be omitted since we only have a single container in the Pod.)

8. Next let’s start a bash session in the Pod’s container:
$ kubectl exec -ti $POD_NAME bash

9. We have now an open console on the container where we run our NodeJS application. The source code of the app is in the server.js file:
$ cat server.js

10. You can check that the application is up by running a curl command:
$ curl localhost:8080
(Note: here we used localhost because we executed the command inside the NodeJS Pod. If you cannot connect to localhost:8080, check to make sure you have run the kubectl exec command and are launching the command from within the Pod)

11. To close your container connection type:
$ exit




---------- D. Expose Your App Publicly ----------

When a worker node dies, the Pods running on the Node are also lost.
Each Pod in a Kubernetes cluster has a unique IP address, even Pods on the same Node.
There needs to be a way of automatically reconciling changes among Pods so that your applications continue to function.
Although each Pod has a unique IP address, those IPs are not exposed outside the cluster without a Service. Services allow your applications to receive traffic. Services can be exposed in different ways by specifying a type in the ServiceSpec:

- ClusterIP (default) - Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster.
- NodePort - Exposes the Service on the same port of each selected Node in the cluster using NAT. Makes a Service accessible from outside the cluster using <NodeIP>:<NodePort>. Superset of ClusterIP.
- LoadBalancer - Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP to the Service. Superset of NodePort.
- ExternalName - Exposes the Service using an arbitrary name (specified by externalName in the spec) by returning a CNAME record with the name. No proxy is used. This type requires v1.7 or higher of kube-dns.

There are some use cases with Services that involve not defining selector in the spec. A Service created without selector will also not create the corresponding Endpoints object. This allows users to manually map a Service to specific endpoints. Another possibility why there may be no selector is you are strictly using type: ExternalName.

A Service routes traffic across a set of Pods. Services are the abstraction that allow pods to die and replicate in Kubernetes without impacting your application. Discovery and routing among dependent Pods (such as the frontend and backend components in an application) is handled by Kubernetes Services.

Services match a set of Pods using labels and selectors, a grouping primitive that allows logical operation on objects in Kubernetes. Labels are key/value pairs attached to objects and can be used in any number of ways:

- Designate objects for development, test, and production
- Embed version tags
- Classify an object using tags

Labels can be attached to objects at creation time or later on. They can be modified at any time. Let's expose our application now using a Service and apply some labels.


1. To look for existing Pods:
$ kubectl get pods

2. To list the current Services from our cluster:
$ kubectl get services

3. To create a new service and expose it to external traffic we'll use the expose command with NodePort as parameter (minikube does not support the LoadBalancer option yet).
$ kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080

4. Run the get services command again:
$ kubectl get services

5. To find out what port was opened externally (by the NodePort option) we'll run the describe service command:
$ kubectl describe services/kubernetes-bootcamp

6. To create an environment variable called NODE_PORT that has the value of the Node port assigned:
$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
$ echo NODE_PORT=$NODE_PORT

7. Now we can test that the app is exposed outside of the cluster using curl, the IP of the Node and the externally exposed port:
$ curl $(minikube ip):$NODE_PORT

8. The Deployment created automatically a label for our Pod. With describe deployment command you can see the name of the label:
$ kubectl describe deployment

9. Let’s use this label to query our list of Pods. We’ll use the kubectl get pods command with -l as a parameter, followed by the label values:
$ kubectl get pods -l run=kubernetes-bootcamp

10. You can do the same to list the existing services:
$ kubectl get services -l run=kubernetes-bootcamp

11. Get the name of the Pod and store it in the POD_NAME environment variable:
$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
$ echo Name of the Pod: $POD_NAME

12. To apply a new label we use the label command followed by the object type, object name and the new label:
$ kubectl label pod $POD_NAME app=v1
=> This will apply a new label to our Pod (we pinned the application version to the Pod),

13. And we can check it with the describe pod command:
$ kubectl describe pods $POD_NAME
=> We see here that the label is attached now to our Pod. 

14. And we can query now the list of pods using the new label:
$ kubectl get pods -l app=v1

15. To delete Services you can use the delete service command. Labels can be used also here:
$ kubectl delete service -l run=kubernetes-bootcamp

16. Confirm that the service is gone:
$ kubectl get services
=> This confirms that our Service was removed.

17. To confirm that route is not exposed anymore you can curl the previously exposed IP and port:
$ curl $(minikube ip):$NODE_PORT
=> This proves that the app is not reachable anymore from outside of the cluster. 

18. You can confirm that the app is still running with a curl inside the pod:
$ kubectl exec -ti $POD_NAME curl localhost:8080
=> We see here that the application is up. This is because the Deployment is managing the application. To shut down the application, you would need to delete the Deployment as well.




---------- E. Scale Your App ----------

Scaling is accomplished by changing the number of replicas in a Deployment.
Scaling out a Deployment will ensure new Pods are created and scheduled to Nodes with available resources. Scaling will increase the number of Pods to the new desired state.
Running multiple instances of an application will require a way to distribute the traffic to all of them. Services have an integrated load-balancer that will distribute network traffic to all Pods of an exposed Deployment. Services will monitor continuously the running Pods using endpoints, to ensure the traffic is sent only to available Pods.

1. To list your deployments
$ kubectl get deployments

2. To see the ReplicaSet created by the Deployment
$ kubectl get rs
=> Notice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[RANDOM-STRING].
DESIRED displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.
CURRENT displays how many replicas are currently running.

3. To scale the Deployment to 4 replicas.
$ kubectl scale deployments/kubernetes-bootcamp --replicas=4
=> the kubectl scale command, followed by the deployment type, name and desired number of instances

4. To list your Deployments once again
$ kubectl get deployments
=> The change was applied, and we have 4 instances of the application available.

5. To check if the number of Pods changed:
$ kubectl get pods -o wide
=> There are 4 Pods now, with different IP addresses. The change was registered in the Deployment events log.

6. To check the registered change, use the describe command:
$ kubectl describe deployments/kubernetes-bootcamp
=> You can also view in the output of this command that there are 4 replicas now.

7. Let’s check that the Service is load-balancing the traffic. To find out the exposed IP and Port we can use the describe service as we learned in the previously Module:
$ kubectl describe services/kubernetes-bootcamp

8. Create an environment variable called NODE_PORT that has a value as the Node port:
$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
$ echo NODE_PORT=$NODE_PORT

9. Next, we’ll do a curl to the exposed IP and port. Execute the command multiple times:
$ curl $(minikube ip):$NODE_PORT
=> We hit a different Pod with every request. This demonstrates that the load-balancing is working.

10. To scale down the Service to 2 replicas, run again the scale command:
$ kubectl scale deployments/kubernetes-bootcamp --replicas=2

11. To list the Deployments to check if the change was applied:
$ kubectl get deployments
=> The number of replicas decreased to 2.

12. To list the number of Pods:
$ kubectl get pods -o wide
=> This confirms that 2 Pods were terminated.




---------- F. Update Your App ----------

Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. The new Pods will be scheduled on Nodes with available resources.
By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Both options can be configured to either numbers or percentages (of Pods). In Kubernetes, updates are versioned and any Deployment update can be reverted to a previous (stable) version.
Similar to application Scaling, if a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. An available Pod is an instance that is available to the users of the application.
Rolling updates allow the following actions:
- Promote an application from one environment to another (via container image updates)
- Rollback to previous versions
- Continuous Integration and Continuous Delivery of applications with zero downtime

1. To list your deployments:
$ kubectl get deployments

2. To list the running Pods:
$ kubectl get pods

3. To view the current image version of the app, run a describe command against the Pods (look at the Image field)
$ kubectl describe pods

4. To update the image of the application to version 2, use the set image command, followed by the deployment name and the new image version:
$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2
=> The command notified the Deployment to use a different image for your app and initiated a rolling update. 

5. To check the status of the new Pods, and view the old one terminating with the get pods command:
$ kubectl get pods

6. First, let's check that the App is running. To find out the exposed IP and Port we can use describe service:
$ kubectl describe services/kubernetes-bootcamp

7. Create an environment variable called NODE_PORT that has the value of the Node port assigned:
$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
$ echo NODE_PORT=$NODE_PORT

8. A curl to the the exposed IP and port:
$ curl $(minikube ip):$NODE_PORT
=> We hit a different Pod with every request and we see that all Pods are running the latest version (v2)

9. The update can be confirmed also by running a rollout status command:
$ kubectl rollout status deployments/kubernetes-bootcamp

10. To view the current image version of the app, run a describe command against the Pods:
$ kubectl describe pods
=> We run now version 2 of the app (look at the Image field)

11. Let’s perform another update, and deploy image tagged as v10:
$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10

12. Use get deployments to see the status of the deployment:
$ kubectl get deployments
=> And something is wrong… We do not have the desired number of Pods available.

13. List the Pods again:
$ kubectl get pods

14. A describe command on the Pods should give more insights:
$ kubectl describe pods
=> There is no image called v10 in the repository.

15. Let’s roll back to our previously working version. We’ll use the rollout undo command:
$ kubectl rollout undo deployments/kubernetes-bootcamp
=> The rollout command reverted the deployment to the previous known state (v2 of the image). Updates are versioned and you can revert to any previously know state of a Deployment.

16. List again the Pods:
$ kubectl get pods
=> Four Pods are running.

17. Check again the image deployed on the them:
$ kubectl describe pods
=> We see that the deployment is using a stable version of the app (v2). The Rollback was successful.
